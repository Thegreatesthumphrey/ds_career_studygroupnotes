{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDY GROUP - M04S39\n",
    "## Developing a Recommendation Engine in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "You will be able to:\n",
    "* explain approaches to recommendations systems\n",
    "* explain concept of collborative filtering method for recommendation systems\n",
    "* compare/contrast matrix decomposition techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to Recommendation Systems\n",
    "\n",
    "1. Popularity Rankings (non-personalized)\n",
    "\n",
    "    Ex. news websites (CNN, BBC, etc)\n",
    "    \n",
    "2. Classification Algorithms (personalized)\n",
    "\n",
    "    Ex. kNN\n",
    "    \n",
    "3. Recommendation Algorithms\n",
    "\n",
    "    a. Content Based (item focused)\n",
    "    \n",
    "    b. Collaborative Filtering (user or item focused) - MOST POPULAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "* The key idea behind CF is that similar users share the same interest and that similar items are liked by a user. CF is filling the blank (cell) in the utility matrix that a user has not seen/rated before based on the similarity between users or items.\n",
    "    1. Memory Based \n",
    "    \n",
    "        Pros - accurate recommendations\n",
    "        \n",
    "        Cons - high computational cost to keeping all data in memory\n",
    "        \n",
    "    2. Model Based \n",
    "    \n",
    "        Pros - easier to scale (abstraction)\n",
    "        \n",
    "        Cons - recommendations not as accurate \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Decomposition\n",
    "* The idea behind such models is that preferences of a users can be determined by a small number of hidden factors. We can call these factors as Embeddings/low-dimensional hidden factors. \n",
    "\n",
    "    1. Single Value Decomposition (SVD)\n",
    "    \n",
    "$$ A = U\\Sigma V^T$$\n",
    "\n",
    "Where $A$ is an $n x d$ matrix, $U$ is an $(n x r)$ orthogonal matrix, $ùö∫$ is an $(r x r)$ nonnegative rectangular diagonal matrix, and $V$ is an $(r x d)$ orthogonal matrix.\n",
    "$U$ is also referred to as the __left singular vectors__, ùö∫ the __singular values__, and V the __right singular vectors__. \n",
    "\n",
    "SVD decreases the dimension of the utility matrix by extracting its latent factors.\n",
    "\n",
    "Essentially, we map each user and each item into a latent space with lower dimension. Therefore, it helps us better understand the relationship between users and items as they become directly comparable.\n",
    "\n",
    "At root what is SVD trying to do? What other techniques have we learned that do the same thing?\n",
    "\n",
    "    2. Alternating Least Squares (ALS)\n",
    "\n",
    "If we multiply each feature of the user by the corresponding feature of the item and add everything together, this will be a good approximation for the rating the user would give to that item.\n",
    "\n",
    "$$ L = \\sum_{u,i ‚àà  S}(r_{u,i}‚àí x_u^T y_i)^2 + Œª_x \\sum_u||x_u||^2 + Œª_y \\sum_u||y_u||^2$$\n",
    "\n",
    "For ALS minimiztion, we hold one set of latent vectors constant. Let's say we pick the item vectors and take the derivative of the loss function with respect to the other set of vectors (the user vectors). We set the derivative equal to zero and solve for the non-constant vectors (the user vectors).\n",
    "\n",
    "Next comes the alternating part. With our new, solved-for user vectors in hand, we hold them constant, instead, and take the derivative of the loss function with respect to the previously constant vectors (the item vectors).\n",
    "\n",
    "ALS involves alternate back and forth and carry out this two-step dance until convergence.\n",
    "\n",
    "**ALS vs SVD**\n",
    "* ALS is generally less computationally efficient than directly computing the SVD solution. \n",
    "* An interesting results of the SVD decomposition is that we get the complete nested set of low-rank approximations.\n",
    "* ALS only gives you a single rank approximation. So if you wanted a rank 5 and rank 10 decomposition, you would need to run the ALS algorithm twice.\n",
    "* SVD requires that all entries of the matrix be observed. This is not the case for ALS. Similarly, ALS easily generalizes to higher order cases (i.e., tensors) while SVD does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
