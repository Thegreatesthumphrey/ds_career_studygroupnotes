{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDY GROUP - M03S32\n",
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Explain ensemble methods\n",
    "* Explain bagging vs boosting\n",
    "* discuss advantages/disadvantages of ensemble methods\n",
    "* explain function of popular boosting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "What is an ensemble method? - using multiple models to make a prediction\n",
    "\n",
    "How is smoothing related to ensemble model accuracy? - due to averaging multiple models outlier effects are minimized\n",
    "\n",
    "What is subspace sampling? - gives each tree a subset of features to decide on, reduces noise in any individual feature\n",
    "\n",
    "What is bagging (bootstrap aggregation)? - random resampling with replacement, introduces variablility in decision tree formation\n",
    "\n",
    "    * What are some common aggregation methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "What is a random forest model and how does it work? - supervised predictor, makes different decision trees using bootstrapping and subspace sampling, aggregates the results\n",
    "\n",
    "What is the weakness of decision trees that makes their aggregation potentially problematic? - strong/greedy learners, without data variance they would all likely produce same decision points/result\n",
    "\n",
    "How do bagging and subspace sampling solve that problem? - by providing different training data and feature set diversity is introduced which will make the aggregation much more representative of phenomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for Parameter Tuning\n",
    "\n",
    "How does GridSearchCV() work? - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting & Weak Learners\n",
    "\n",
    "strong learner vs weak learner - strong learners try to maximize IG which is reflected in tree depth, weak learners just try to be better than random chance and imporove on weakness with each iteration\n",
    "\n",
    "boosting - interative rather than independent process, using each model to strengthen the weaknesses of the previous ones\n",
    "\n",
    "How does Adaboost work? - \n",
    "\n",
    "How does Gradient Boosting work? - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
