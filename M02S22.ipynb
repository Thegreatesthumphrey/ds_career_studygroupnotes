{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDY GROUP - M02S22\n",
    "## Bayesian Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "You will be able to:\n",
    "* Understand pros and cons of Baye's Theorem\n",
    "* Understand assumptions that makes NB models \"naive\"\n",
    "* Explain different distributions used with NB\n",
    "* Give examples of NB applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised**\n",
    "- starts with example dataset to learn from\n",
    "\n",
    "* Regression: creating a trendline to predict a value\n",
    "\n",
    "* Classifcation: categorization for new data based on training data\n",
    "\n",
    "**Unsupervised**\n",
    "- learns from new data only\n",
    "\n",
    "* Clustering: creating classes from unmarked data based on some measure of proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP Estimation and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before , we use MAP when we have *some* idea about the prior probability of the data. We can calclate marginal probabilities or simply a subjective value for the **prior probability** in a Bayesian context. We can write Bayes' theorem as:\n",
    "\n",
    "$$P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}$$\n",
    "\n",
    "$$ = \\frac{P(X|\\theta)P(\\theta)}{\\sum_\\Theta  P(\\theta,X)}$$\n",
    "\n",
    "$$ = \\frac{P(X|\\theta)P(\\theta)}{\\sum_\\Theta  P(X|\\theta)P(\\theta)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption 1: Feature independence**\n",
    "\n",
    "**Assumption 2: Data Distribution**\n",
    "\n",
    "* Gaussian NB\n",
    "- nomrally distributed, continuous data\n",
    "* Multinomial NB\n",
    "- discrete distributions, counts\n",
    "* Bernoulli NB\n",
    "- Great for 2 class problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pro and cons of Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "\n",
    "* Computationally fast\n",
    "* Simple to implement\n",
    "* Works well with small datasets\n",
    "* Works well with high dimensions\n",
    "* Perform well even if the Naive Assumption is not perfectly met. In many cases, the approximation is enough to build a good classifier.\n",
    "* They provide straightforward probabilistic prediction\n",
    "* They are often very easily interpretable\n",
    "* They have very few (if any) tunable parameters\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Require to remove correlated features because they are voted twice in the model and it can lead to over inflating importance.\n",
    "* If a categorical variable has a category in test data set which was not observed in training data set, then the model will assign a zero probability. It will not be able to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation. Sklearn applies Laplace smoothing by default when you train a Naive Bayes classifier.\n",
    "\n",
    "\n",
    "**Naive Bayes classifiers tend to perform especially well in one of the following situations:**\n",
    "\n",
    "* When the naive assumptions actually match the data (very rare in practice)\n",
    "* For very well-separated categories, when model complexity is less important\n",
    "* For very high-dimensional data, when model complexity is less important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html\n",
    "\n",
    "https://blog.sicara.com/naive-bayes-classifier-sklearn-python-example-tips-42d100429e44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
