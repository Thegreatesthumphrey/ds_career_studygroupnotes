{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDY GROUP - M04S42\n",
    "## Regularization and Tuning in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "You will be able to:\n",
    "* explain relationship between bias/variance and underfit/overfit\n",
    "* describe the different regularization methods\n",
    "* illustrate why normalization is important in deep learning\n",
    "* describe optimization alternatives to SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias/Variance Tradeoff in Deep Learning\n",
    "\n",
    "What's the relationship between bias/variance and underfit/overfit?\n",
    "- bias:underfit :: variance:overfit\n",
    "\n",
    "Solutions for underfit?\n",
    "* add layers, epochs\n",
    "\n",
    "Solutions for overfit?\n",
    "* more data, fewer layers/epochs, regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "- penalty for coefficients/weights to prevent overfit/variance\n",
    "* L1/lasso - abs(W), allows for feature/weight elimation as well as regularization\n",
    "* L2/ridge - W^2, allows for minimization/regularization of features/weights\n",
    "* Dropout - randomly selecting subset of nodes to be dropped per training epoch to better generalize feature knowledge and prevent covariance from developing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization & Tuning\n",
    "\n",
    "- Why normalize inputs?\n",
    "* exploding gradients and scaling differences can cause either broken models or extreme computation times. normalization prevents these outcomes by standardinzing data scaling.\n",
    "\n",
    "Optimization Methods:\n",
    "* Gradient Descent with Momentum\n",
    "- weighted average of previous gradients to minimze oscillation and converge on minimum more quickly\n",
    "* RMSProp\n",
    "- slow down learning on one direction and speed up in another one\n",
    "* ADAM\n",
    "- \"adaptive moment estimation\", taking momentum and RMSprop and putting it together\n",
    "* Learning Rate Decay\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
